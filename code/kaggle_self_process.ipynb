{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from category_encoders import TargetEncoder\n",
    "from tsfresh import extract_features\n",
    "# ComprehensiveFCParameters\n",
    "from tsfresh.feature_extraction import MinimalFCParameters\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from itertools import product\n",
    "from typing import List\n",
    "import pymssql\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "import base64\n",
    "import traceback\n",
    "# %autosave 10\n",
    "\n",
    "import yaml\n",
    "import os\n",
    "import logging\n",
    "from log.logging_setting import Log\n",
    "from util import cfg\n",
    "\n",
    "Log().create_logger('NP_Txn.log')\n",
    "logger = logging.getLogger('NP_Txn.log')\n",
    "\n",
    "# from IPython import embed\n",
    "# %%\n",
    "with open(os.path.join(os.path.dirname(__file__), 'config.yaml'), encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "snapshot_folder = os.path.join(os.path.dirname(__file__), 'snapshot_file_txn')\n",
    "# %%\n",
    "if not os.path.isdir(snapshot_folder):\n",
    "    os.mkdir(snapshot_folder)\n",
    "\n",
    "# sys.path.append('..')\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", pd.errors.DtypeWarning)\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "class NonDataError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Nonnextweek(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def connect_db():\n",
    "    return pymssql.connect(\n",
    "        server=cfg['DB_ETL']['S'], \n",
    "        database=cfg['DB_ETL']['N'])\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, tgt_date, scenario_name, group, level, bankL=True, is_Train=True):\n",
    "        logger.info(f'------------Dataset--------------  \\\n",
    "                    \\n scenario name: {scenario_name}   \\\n",
    "                    \\n tgt_date: {tgt_date}\\\n",
    "                    \\n group: {group}\\\n",
    "                    \\n is_Train: {is_Train}\\\n",
    "                    \\n---------------------------------')\n",
    "        assert group in [\"Person\", \"Corp\"]\n",
    "        assert level in [\"PTY\", \"ACC\"]\n",
    "\n",
    "        self.tgt_date = tgt_date  # 計算基期\n",
    "        self.scenario_name = scenario_name\n",
    "        self.group = group\n",
    "        self.level = level\n",
    "        self.is_Train = is_Train\n",
    "        self.n_days = 7\n",
    "        self.rd_ndays = f\"資料日期-{self.n_days}日\"\n",
    "        self.cust_lists = {}\n",
    "        self.entity_level = \"客戶編碼\" if level == \"PTY\" else \"帳戶編碼\"\n",
    "\n",
    "        self._tgt_group_info = None\n",
    "        self.vip_list = None\n",
    "        self.train_start_date = None\n",
    "        self.train_end_date = self.tgt_date - timedelta(days=365)  # 訓練資料結束日\n",
    "        alert, txn, risk, info, ac = self._get_data_from_sql(\n",
    "        ) if bankL else self._get_bankT_data()  # 從DB讀取資料\n",
    "\n",
    "        self._alert_processed = (\n",
    "            alert\n",
    "            .drop([\"警示編號\", \"監控層級\", \"觸發說明\"], axis=1)\n",
    "            .assign(**alert.資料日期.dt.isocalendar(),\n",
    "                    month=alert.資料日期.dt.month,\n",
    "                    quarter=alert.資料日期.dt.quarter,\n",
    "                    hfy=alert.資料日期.dt.quarter.isin([3, 4])+1,\n",
    "                    bigMon=alert.資料日期.dt.month.isin([1, 3, 5, 7, 8, 10, 12]).astype(int))  # 將alert整理出更多的特徵資料\n",
    "        )\n",
    "        self.txn_c = txn[txn['tran_type'] == 'DEBIT'].drop(\"tran_type\", axis=1)\n",
    "        self.txn_d = txn[txn['tran_type'] ==\n",
    "                         'CREDIT'].drop(\"tran_type\", axis=1)\n",
    "        self.txn_t = txn[txn['tran_type'] == 'TXN'].drop(\"tran_type\", axis=1)\n",
    "\n",
    "        self._txn_processed = txn  # _get_data_from_sql整理出的txn資料\n",
    "        self._risk_processed = risk  # _get_data_from_sql整理出的risk資料\n",
    "        self._info_processed = info  # _get_data_from_sql整理出的info資料\n",
    "        self.ac = ac  # _get_data_from_sql從VW_NP_FSC_PARTY_ACCOUNT_BRIDGE整理出的客帳戶對照\n",
    "\n",
    "        self.train = self._get_data(\"train\")  # VIP訓練資料\n",
    "        self.val = self._get_data(\"val\")  # VIP驗證資料\n",
    "        self.test = self._get_data(\"test\")  # VIP當天資料\n",
    "\n",
    "        self.seg_mapping = self._get_cust_seg_def()\n",
    "\n",
    "    def num_sam_dist(self, key: List[str], on: str, cust_seg: str = \"\"):\n",
    "        cust_list = self.cust_seg_cust_list(cust_seg) if cust_seg else None\n",
    "        if on == \"all\":\n",
    "            return (\n",
    "                self._alert_processed\n",
    "                .pipe(lambda x, cust_list=cust_list: x.query(\"客戶編碼 in @cust_list\") if cust_seg else x)\n",
    "                .groupby(key)\n",
    "                .agg({\"資料日期\": \"count\"})\n",
    "                .rename({\"資料日期\": \"警示數量\"}, axis=1)\n",
    "                .reset_index()\n",
    "            )\n",
    "        elif on == \"vip\":\n",
    "            return (\n",
    "                self._alert_processed\n",
    "                .pipe(lambda x, cust_list=cust_list: x.query(\"客戶編碼 in @cust_list\") if cust_seg else x)\n",
    "                .query(f\"{self.entity_level} in @self.vip_list\")\n",
    "                .groupby(key)\n",
    "                .agg({\"資料日期\": \"count\"})\n",
    "                .rename({\"資料日期\": \"警示數量\"}, axis=1)\n",
    "                .reset_index()\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Unknown Target data\")\n",
    "\n",
    "    def num_sam_dist2(self, key: List[str], on: str, cust_seg: str = \"\"):\n",
    "        if on == \"all\":\n",
    "            return (\n",
    "                self._alert_processed\n",
    "                .pipe(lambda x, seg_list=self.seg_mapping[cust_seg]: x.query(\"cust_segmentation in @seg_list\") if cust_seg else x)\n",
    "                .groupby(key)\n",
    "                .agg({\"資料日期\": \"count\"})\n",
    "                .rename({\"資料日期\": \"警示數量\"}, axis=1)\n",
    "                .reset_index()\n",
    "            )\n",
    "        elif on == \"vip\":\n",
    "            return (\n",
    "                self._alert_processed\n",
    "                .pipe(lambda x, seg_list=self.seg_mapping[cust_seg]: x.query(\"cust_segmentation in @seg_list\") if cust_seg else x)\n",
    "                .query(f\"{self.entity_level} in @self.vip_list\")\n",
    "                .groupby(key)\n",
    "                .agg({\"資料日期\": \"count\"})\n",
    "                .rename({\"資料日期\": \"警示數量\"}, axis=1)\n",
    "                .reset_index()\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Unknown Target data\")\n",
    "\n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    # ------------------------ 取 bankT 資料 ------------------------\n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "    # 假設 alert 裡只有一個態樣\n",
    "    # 假設 alert 裡的客戶編碼只有自然人 or 只有法人\n",
    "    # alert 裡的客戶編碼 = cust_info 裡的客戶編碼\n",
    "    # tgt_date 請填入 pd.to_datetime(\"2020-12-27\"), 不要填入字串日期\n",
    "\n",
    "    # def _get_bankT_data(self):\n",
    "    #     if self.level == \"PTY\":\n",
    "    #         alert = (\n",
    "    #             pd.read_csv('D:/Data_/A_cn.csv', index_col=0,\n",
    "    #                         parse_dates=[\"資料日期\"])\n",
    "    #             .query(\"監控層級 == 'PTY'\")\n",
    "    #             .drop(\"監控客(帳)戶號碼\", axis=1)\n",
    "    #         )\n",
    "    #     else:\n",
    "    #         alert = (\n",
    "    #             pd.read_csv('D:/Data_/A_cn.csv', index_col=0,\n",
    "    #                         parse_dates=[\"資料日期\"])\n",
    "    #             .query(\"監控層級 == 'ACC'\")\n",
    "    #             .rename({\"監控客(帳)戶號碼\": \"帳戶編碼\"}, axis=1)\n",
    "    #         )\n",
    "\n",
    "    #     cust_info = pd.read_csv(\n",
    "    #         'D:/Data_/mCp.csv', parse_dates=[\"資料開始週期\"], index_col=0)\n",
    "    #     self._tgt_group_info = cust_info[['客戶編碼', '客戶分群', '資料開始週期']]\n",
    "\n",
    "    #     # ------------------------ 加入客戶分群 ------------------------\n",
    "    #     alert = self._alert_add_cust_seg(alert)\n",
    "    #     # ------------------------ 新的計算 VIP 方式 ------------------------\n",
    "    #     self._alert_processed = alert\n",
    "    #     # self.train_start_date = max(\n",
    "    #     #     self.tgt_date - timedelta(days=365*2), alert.資料日期.min())  # 訓練資料起始日\n",
    "    #     self.train_start_date = self.tgt_date - timedelta(days=365*2)\n",
    "\n",
    "    #     self.vip_list = self._get_vip_list()\n",
    "\n",
    "    #     if self.level == \"PTY\":\n",
    "    #         ac = None\n",
    "    #         cust_info = cust_info.query(\"客戶編碼 in @self.vip_list\")\n",
    "    #     else:\n",
    "    #         ac = (\n",
    "    #             pd.read_csv(\"D:/from210/210to230/NCTU/share/210/Data/AC.csv\",\n",
    "    #                         sep='\\t', usecols=['Acct_No', 'Cust_No', 'Change_Begin_Dt'], parse_dates=['Change_Begin_Dt'])\n",
    "    #             .rename({'Acct_No': '帳戶編碼',\n",
    "    #                      'Cust_No': '客戶編碼',\n",
    "    #                      'Change_Begin_Dt': '資料開始週期'}, axis=1)\n",
    "    #             .query(\"帳戶編碼 in @self.vip_list\")\n",
    "    #             .sort_values(\"資料開始週期\")\n",
    "    #         )\n",
    "    #         cust_info = cust_info.query(\"客戶編碼 in @ac.客戶編碼.unique()\")\n",
    "\n",
    "    #     txn = (\n",
    "    #         pd.read_csv('D:/Data_/Txn_cn.csv', parse_dates=[\"交易日期\"], usecols=[\n",
    "    #                     \"交易日期\", '折台幣交易金額', \"客戶編碼\", \"交易類型識別碼\", \"帳戶號碼\"])\n",
    "    #         .rename({\"帳戶號碼\": \"帳戶編碼\"}, axis=1)\n",
    "    #         .query(f\"{self.entity_level} in @self.vip_list\")\n",
    "    #     )\n",
    "\n",
    "    #     return alert, txn, cust_info, cust_info, ac\n",
    "\n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    # ------------------------ 取 bankT 資料 ------------------------\n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    # ------------------------ 取 bankL 資料 ------------------------\n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "    def _get_data_from_sql(self):\n",
    "\n",
    "        def res_to_df(res, cur_des):\n",
    "            return pd.DataFrame(res, columns=[cur_des[i][0] for i in range(len(cur_des))])\n",
    "        f = open('test.txt', 'w', encoding='utf-8')\n",
    "        conn = connect_db()\n",
    "\n",
    "        cur = conn.cursor()\n",
    "        # //------------------------ 讀入客戶警示檔 ------------------------\n",
    "        # cur.execute(\"SELECT * FROM \" +\n",
    "        #             f\"VW_NP_FSK_ALERT_{self.scenario_name[-5:-2] + '_' + self.scenario_name[-2:]}\")  # 從對應樣態的alert table中找出所有的alert(不只訓練資料而是權範圍)為何沒有限制?\n",
    "\n",
    "        # f\"VW_NP_FSK_ALERT_{self.scenario_name[-5:-2] + '_' + self.scenario_name[-2:]}\")\n",
    "        if self.level == \"PTY\":\n",
    "\n",
    "            cur.execute(\"SELECT alert_id\\\n",
    "                    , primary_entity_level_code\\\n",
    "                    , primary_entity_number\\\n",
    "                    , scenario_name\\\n",
    "                    , actual_values_text\\\n",
    "                    , run_date\\\n",
    "                    , cust_segmentation\\\n",
    "                    , Cust_No as party_number FROM \" +\n",
    "                        f\"VW_FB_Alert_{self.group} with (nolock) where scenario_name='TWN_{self.scenario_name[-5:-2] + '_' + self.scenario_name[-2:]}'\")\n",
    "\n",
    "            alert = (\n",
    "                res_to_df(cur.fetchall(), cur.description)\n",
    "                .query(\"primary_entity_level_code == 'PTY'\")\n",
    "                .drop(\"primary_entity_number\", axis=1)\n",
    "                .rename({\"party_number\": \"客戶編碼\"}, axis=1)\n",
    "                .rename({\"party_number\": \"客戶編碼\"}, axis=1)\n",
    "            )\n",
    "            alert[\"客戶編碼\"] = alert[\"客戶編碼\"].apply(\n",
    "                lambda x: x.replace(' ', ''))\n",
    "\n",
    "        else:\n",
    "            cur.execute(\"SELECT alert_id\\\n",
    "                    , primary_entity_level_code\\\n",
    "                    , primary_entity_number as Acct_Num\\\n",
    "                    , scenario_name\\\n",
    "                    , actual_values_text\\\n",
    "                    , run_date\\\n",
    "                    , cust_segmentation\\\n",
    "                    , Cust_No as party_number FROM \" +\n",
    "                        f\"VW_FB_Alert_{self.group} with (nolock) where scenario_name='TWN_{self.scenario_name[-5:-2] + '_' + self.scenario_name[-2:]}'\")\n",
    "            alert = (\n",
    "                res_to_df(cur.fetchall(), cur.description)\n",
    "                .query(\"primary_entity_level_code == 'ACC'\")\n",
    "                .rename({\"Acct_Num\": \"帳戶編碼\",\n",
    "                         \"party_number\": \"客戶編碼\"}, axis=1)\n",
    "            )\n",
    "            alert[\"客戶編碼\"] = alert[\"客戶編碼\"].apply(\n",
    "                lambda x: x.replace(' ', ''))\n",
    "            alert[\"帳戶編碼\"] = alert[\"帳戶編碼\"].apply(\n",
    "                lambda x: x.replace(' ', ''))\n",
    "        if alert.shape[0] == 0:\n",
    "            raise NonDataError('No alert')  # 沒有alert直接預測都是0合理\n",
    "        logger.info(f'alert part number {alert.客戶編碼.unique()}')\n",
    "        # logger.info(\n",
    "        #     f'PB081001018795 in alert {\"PB081001018795\" in alert.帳戶編碼.values.tolist()}')\n",
    "        # logger.info(\n",
    "        # f'*2-5.059 in alert {\"*2-5.059\" in alert.客戶編碼.values.tolist()}')\n",
    "        alert.to_csv(os.path.join(snapshot_folder, 'alert1.csv'))\n",
    "        f.write('alert \\n')\n",
    "        f.write('  '.join(alert.客戶編碼.unique()))\n",
    "        # ------------------------ 讀入客戶警示檔 ------------------------//\n",
    "        # from IPython import embed\n",
    "        # embed()\n",
    "        # //------------------------ 抓取客戶資料------------------------\n",
    "        #\n",
    "        cur.execute(\"SELECT [party_number], [change_begin_date], [customer_segmentation] FROM \" + f\"VW_NP_FSC_PARTY_DIM_{self.group} with (nolock)\" +\n",
    "                    \" where party_number in %s\" % str(tuple(alert.客戶編碼.unique())))   # 從alert找出對應的客戶編碼並在party dim找出對應的客戶資料\n",
    "\n",
    "        self._tgt_group_info = (\n",
    "            res_to_df(cur.fetchall(), cur.description)\n",
    "            .rename({\"change_begin_date\": \"資料開始週期\",\n",
    "                     \"party_number\": \"客戶編碼\",\n",
    "                     \"customer_segmentation\": \"客戶分群\"}, axis=1)  # cust_seg_cust_list來源\n",
    "        )\n",
    "\n",
    "        logger.info(\n",
    "            f'_tgt_group_info part number {self._tgt_group_info.客戶編碼.unique()}')\n",
    "        # logger.info(\n",
    "        # f'*2-5.059 in alert {\"*2-5.059\" in self._tgt_group_info.客戶編碼.values.tolist()}')\n",
    "        self._tgt_group_info.to_csv(os.path.join(\n",
    "            snapshot_folder, 'tgt_group_info.csv'))\n",
    "        f.write('\\n_tgt_group_info \\n')\n",
    "        f.write('  '.join(self._tgt_group_info.客戶編碼.unique()))\n",
    "        if alert.shape[0] == 0:\n",
    "            raise NonDataError('No alert')  # 這部分找不到對應資料可能要再確認問題\n",
    "        # ------------------------ 抓取客戶資料------------------------//\n",
    "\n",
    "        # //-----------------------保留alert內客戶資料有在客戶表內的alert-------------------------\n",
    "        alert = (\n",
    "            alert\n",
    "            .rename({\"run_date\": \"資料日期\",\n",
    "                     \"alert_id\": \"警示編號\",\n",
    "                     \"primary_entity_level_code\": \"監控層級\",\n",
    "                     \"actual_values_text\": \"觸發說明\"}, axis=1)\n",
    "            # 過濾出自然人/法人客戶產生的警示\n",
    "            .query(\"客戶編碼 in @self._tgt_group_info.客戶編碼.unique()\")\n",
    "        )\n",
    "\n",
    "        logger.info(\n",
    "            f'alert part number {alert.客戶編碼.unique()}')\n",
    "        # logger.info(\n",
    "        #     f'PB081001018795 in alert {\"PB081001018795\" in alert.帳戶編碼.values.tolist()}')\n",
    "        # logger.info(\n",
    "        # f'*2-5.059 in alert {\"*2-5.059\" in alert.客戶編碼.values.tolist()}')\n",
    "        f.write('\\nalert \\n')\n",
    "        f.write('  '.join(alert.客戶編碼.unique()))\n",
    "        # ------------------------ 加入客戶分群 ------------------------\n",
    "        alert = self._alert_add_cust_seg(alert)  # 將alert mapping 客戶表標註alert的\n",
    "        # ------------------------ 新的計算 VIP 方式 ------------------------\n",
    "        self._alert_processed = alert\n",
    "        logger.info(\n",
    "            f'_alert_processed part number {self._alert_processed.客戶編碼.unique()}')\n",
    "        # logger.info(\n",
    "        #     f'PB081001018795 in alert {\"PB081001018795\" in self._alert_processed.帳戶編碼.values.tolist()}')\n",
    "        # logger.info(\n",
    "        # f'*2-5.059 in alert {\"*2-5.059\" in self._alert_processed.客戶編碼.values.tolist()}')\n",
    "        f.write('\\n _alert_processed \\n')\n",
    "        f.write('  '.join(self._alert_processed.客戶編碼.unique()))\n",
    "        f.close\n",
    "        # if self.is_Train:\n",
    "        #     self.train_start_date = max(\n",
    "        #         self.tgt_date - timedelta(days=365*2), alert.資料日期.min())  # 訓練資料起始日\n",
    "        # else:\n",
    "        self.train_start_date = self.tgt_date - timedelta(days=365*2)\n",
    "        self._alert_processed.to_csv(os.path.join(\n",
    "            snapshot_folder, 'alert_proceessed.csv'))\n",
    "        self.vip_list = self._get_vip_list()\n",
    "        logger.info(f'_get_vip_list  {self.vip_list}')\n",
    "\n",
    "        if self.level == \"PTY\":\n",
    "            print('PTY')\n",
    "            ac = None\n",
    "            # 使用VIP清單回頭查詢客戶資料\n",
    "            cur.execute(\n",
    "                \"SELECT * FROM \" + f\"VW_NP_FSC_PARTY_DIM_{self.group} with (nolock)\" + \" where party_number in %s\" % str(tuple(self.vip_list)))\n",
    "            cust_info = (\n",
    "                res_to_df(cur.fetchall(), cur.description)\n",
    "                .rename({\"change_begin_date\": \"資料開始週期\",\n",
    "                        \"party_number\": \"客戶編碼\",\n",
    "                         \"customer_segmentation\": \"客戶分群\"}, axis=1)\n",
    "                .drop(\"change_end_date\", axis=1)\n",
    "            )\n",
    "            # 使用VIP回頭查詢交易明細\n",
    "            cur.execute(\"SELECT [Posted_Date_Key], [Cust_Num], [Ccy_Amt] ,[transaction_cdi_desc]\\\n",
    "                        FROM \" + f\"VW_NP_FB_Scenario_{self.scenario_name} with (nolock)\" + \" where Cust_Num in %s\" % str(tuple(self.vip_list)))\n",
    "            txn = (\n",
    "                res_to_df(cur.fetchall(), cur.description)\n",
    "                .rename({\"Posted_Date_Key\": \"交易日期\",\n",
    "                        \"Cust_Num\": \"客戶編碼\",\n",
    "                         \"Ccy_Amt\": \"折台幣交易金額\",\n",
    "                         \"transaction_cdi_desc\": \"tran_type\"}, axis=1)\n",
    "                .astype({\"交易日期\": str})\n",
    "                .pipe(lambda x: x.assign(交易日期=pd.to_datetime(x.交易日期)))\n",
    "            )\n",
    "\n",
    "            # 使用VIP回頭查詢客戶風險檔\n",
    "\n",
    "            query = \"SELECT * FROM \" + \\\n",
    "                f\"VW_NP_FSC_PARTY_RISK_FACTOR_DIM_SCD_{self.group} with (nolock)\" + \\\n",
    "                    \" where party_number in %s\" % str(tuple(self.vip_list))\n",
    "            logger.info(\n",
    "                f'{self.tgt_date} vip get risk table \\n query\\n {query}')\n",
    "            cur.execute(query)\n",
    "            risk = (\n",
    "                res_to_df(cur.fetchall(), cur.description)\n",
    "                .rename({\"change_begin_date\": \"資料開始週期\",\n",
    "                         \"party_number\": \"客戶編碼\"}, axis=1)\n",
    "                .drop(\"change_end_date\", axis=1)\n",
    "                .pipe(lambda x: x.assign(資料開始週期=pd.to_datetime(x.資料開始週期)))\n",
    "            )\n",
    "            if risk.shape[0] == 0:\n",
    "                raise NonDataError('no risk data')\n",
    "\n",
    "        else:\n",
    "            print('ACC')\n",
    "            # 使用vip回頭查克帳戶對照\n",
    "            cur.execute(\"SELECT [account_number], [party_number], [change_begin_date] FROM VW_NP_FSC_PARTY_ACCOUNT_BRIDGE with (nolock)\" +\n",
    "                        \" where account_number in %s\" % str(tuple(self.vip_list)))\n",
    "            ac = (\n",
    "                res_to_df(cur.fetchall(), cur.description)\n",
    "                .rename({\"change_begin_date\": \"資料開始週期\",\n",
    "                        \"party_number\": \"客戶編碼\",\n",
    "                         \"account_number\": \"帳戶編碼\"}, axis=1)\n",
    "            )\n",
    "\n",
    "            if ac.shape[0] == 0:\n",
    "                raise NonDataError('no ac')\n",
    "            # 使用查到的客帳戶對照找到客戶資料\n",
    "            cur.execute(\"SELECT * FROM \" + f\"VW_NP_FSC_PARTY_DIM_{self.group} with (nolock)\" +\n",
    "                        \" where party_number in %s\" % str(tuple(ac.客戶編碼.unique())))\n",
    "            cust_info = (\n",
    "                res_to_df(cur.fetchall(), cur.description)\n",
    "                .rename({\"change_begin_date\": \"資料開始週期\",\n",
    "                        \"party_number\": \"客戶編碼\",\n",
    "                         \"customer_segmentation\": \"客戶分群\"}, axis=1)\n",
    "                .drop(\"change_end_date\", axis=1)\n",
    "            )\n",
    "            # 使用vip回頭找交易明細\n",
    "            cur.execute(\"SELECT [Posted_Date_Key], [Acct_Num], [Ccy_Amt],[transaction_cdi_desc]\\\n",
    "                        FROM \" + f\"VW_NP_FB_Scenario_{self.scenario_name}\" + \" with (nolock) where Acct_Num in %s\" % str(tuple(self.vip_list)))\n",
    "            txn = (\n",
    "                res_to_df(cur.fetchall(), cur.description)\n",
    "                .rename({\"Posted_Date_Key\": \"交易日期\",\n",
    "                        \"Cust_Num\": \"客戶編碼\",\n",
    "                         \"Acct_Num\": \"帳戶編碼\",\n",
    "                         \"Ccy_Amt\": \"折台幣交易金額\",\n",
    "                         \"transaction_cdi_desc\": \"tran_type\"}, axis=1)\n",
    "                .astype({\"交易日期\": str})\n",
    "                .pipe(lambda x: x.assign(交易日期=pd.to_datetime(x.交易日期)))\n",
    "            )\n",
    "            # 使用VIP回頭找客戶風險\n",
    "            cur.execute(\n",
    "                \"SELECT * FROM \" + f\"VW_NP_FSC_PARTY_RISK_FACTOR_DIM_SCD_{self.group}\" + \" with (nolock) where party_number in %s\" % str(tuple(ac.客戶編碼.unique())))\n",
    "            risk = (\n",
    "                res_to_df(cur.fetchall(), cur.description)\n",
    "                .rename({\"change_begin_date\": \"資料開始週期\",\n",
    "                         \"party_number\": \"客戶編碼\"}, axis=1)\n",
    "                .drop(\"change_end_date\", axis=1)\n",
    "                .pipe(lambda x: x.assign(資料開始週期=pd.to_datetime(x.資料開始週期)))\n",
    "            )\n",
    "            if risk.shape[0] == 0:\n",
    "                raise NonDataError('no risk data')\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "        return alert, txn, risk, cust_info, ac\n",
    "\n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    # ------------------------ 取 bankL 資料 ------------------------\n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "    def _get_cust_seg_def(self):\n",
    "        #         def_mapping = {\n",
    "        #             '個人高風險': ['個人高風險'],\n",
    "        #             '個人非高風險': ['個人非高風險'],\n",
    "        #         }\n",
    "        if self.group == \"Person\":\n",
    "            def_mapping = {\n",
    "                'PEH': ['PE', 'PH'],\n",
    "                'PML': ['PM', 'PL', 'P'],\n",
    "            }\n",
    "        else:\n",
    "            def_mapping = {\n",
    "                'CEH1': ['CE1', 'CH1'],\n",
    "                'CML1': ['CM1', 'CL1'],\n",
    "                'CEH2': ['CE2', 'CH2'],\n",
    "                'CML2': ['CM2', 'CL2'],\n",
    "                'CEH3': ['CE3', 'CH3'],\n",
    "                'CML3': ['CM3', 'CL3'],\n",
    "                'C4': ['CE4', 'CH4', 'CM4', 'CL4']\n",
    "            }\n",
    "\n",
    "        def_mapping.update(\n",
    "            {\"all\": [item for l in def_mapping.values() for item in l]})\n",
    "\n",
    "        return def_mapping\n",
    "\n",
    "    def _alert_add_cust_seg(self, alert):\n",
    "        '''\n",
    "        使用該客戶資料中最新的客戶風險分群作為該客戶所有資料的客戶風險分群\n",
    "        '''\n",
    "        #         return (\n",
    "        #             alert\n",
    "        #             .set_index(\"警示編號\")\n",
    "        #             .pipe(lambda x: x.assign(客戶分群=(\n",
    "        #                 x\n",
    "        #                 .reset_index()\n",
    "        #                 .merge(self._tgt_group_info, how=\"left\")\n",
    "        #                 .pipe(lambda y: y.loc[\n",
    "        #                     y\n",
    "        #                     .query(\"資料開始週期 < 資料日期\")\n",
    "        #                     .groupby(\"警示編號\")[\"資料開始週期\"]\n",
    "        #                     .idxmax()])\n",
    "        #                 .set_index(\"警示編號\")\n",
    "        #                 .客戶分群\n",
    "        #             )))\n",
    "        #             .reset_index()\n",
    "        #         )\n",
    "\n",
    "        return (\n",
    "            alert\n",
    "            .assign(客戶分群=(\n",
    "                alert\n",
    "                .客戶編碼\n",
    "                .map((\n",
    "                    self._tgt_group_info\n",
    "                    .astype({\"資料開始週期\": str})\n",
    "                    .set_index([\"客戶編碼\", \"資料開始週期\"])\n",
    "                    .loc[(\n",
    "                        self._tgt_group_info\n",
    "                        .groupby(\"客戶編碼\")\n",
    "                        .agg({\"資料開始週期\": \"max\"})\n",
    "                        .astype({\"資料開始週期\": str})\n",
    "                        .to_records()\n",
    "                        .tolist()\n",
    "                    )]\n",
    "                    .reset_index()\n",
    "                )\n",
    "                    .pipe(lambda x: dict(zip(x.客戶編碼, x.客戶分群)))\n",
    "                )))\n",
    "        )\n",
    "\n",
    "    def _get_vip_list(self):\n",
    "        \"\"\"\n",
    "        取得VIP清單(Description)\n",
    "        如果是is_Train=True用時，資料會參考self.train_start_date 和 self.train_end_date\n",
    "        反之則使用_alert_processed所有的entity進行VIP計算\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        index_array : ndarray\n",
    "            所有類群的VIP清單\n",
    "\n",
    "        See Also\n",
    "        --------\n",
    "        (Description)\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        (Description)\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> \n",
    "        \"\"\"\n",
    "        def to_valid_v(v):\n",
    "            if v < 3:\n",
    "                return 3\n",
    "            else:\n",
    "                return v\n",
    "        if self.is_Train:\n",
    "            logger.info(\n",
    "                f'data in train, self entity level {self.entity_level}')\n",
    "            return np.hstack(\n",
    "                self._alert_processed.query(\n",
    "                    \"@self.train_start_date <= 資料日期 < @self.train_end_date\")  # 用train範圍查找vip\n",
    "                .groupby([\"客戶分群\"])  # groupby客戶分群\n",
    "                .apply(lambda x: (\n",
    "                    # 對每個分群又進行了基於entity_level(客戶編碼或帳戶編碼)的groupby\n",
    "                    x.groupby([self.entity_level])\n",
    "                    .agg({\"資料日期\": \"count\"})  # 計算每個entity底下共有多少筆\n",
    "                    .rename({\"資料日期\": \"警示數量\"}, axis=1)  # 轉換欄位名稱\n",
    "                    .reset_index()  # 移除groupby後的Index(注意，這邊沒有移除基於客戶分群的編碼)\n",
    "                    .pipe(lambda vip_sam_dist: (  # 對個別分群的客戶，計算其數量，取前1%作為VIP，若不足3\n",
    "                        vip_sam_dist\n",
    "                        .nlargest(to_valid_v(int(len(vip_sam_dist) * 0.01)), \"警示數量\")\n",
    "                        [self.entity_level]\n",
    "                        .unique()\n",
    "                    ))))\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\n",
    "                f'data in inference, self entity level {self.entity_level}')\n",
    "            return np.hstack(\n",
    "                # .query(\"@self.train_start_date <= 資料日期 < @self.train_end_date\")#用train範圍查找vip\n",
    "                self._alert_processed.query(\"@self.train_start_date <= 資料日期\")\n",
    "                .groupby([\"客戶分群\"])\n",
    "                    .apply(lambda x: (\n",
    "                        x.groupby([self.entity_level])\n",
    "                        .agg({\"資料日期\": \"count\"})\n",
    "                        .rename({\"資料日期\": \"警示數量\"}, axis=1)\n",
    "                        .reset_index()\n",
    "                        .pipe(lambda vip_sam_dist: (\n",
    "                            vip_sam_dist\n",
    "                            .nlargest(to_valid_v(int(len(vip_sam_dist) * 0.01)), \"警示數量\")\n",
    "                            [self.entity_level]\n",
    "                            .unique()\n",
    "                        ))))\n",
    "            )\n",
    "\n",
    "    def cust_seg_cust_list(self, cust_seg: str):\n",
    "        if cust_seg not in self.cust_lists:\n",
    "            cust_seg_list = self.seg_mapping.get(cust_seg, cust_seg)\n",
    "\n",
    "            self.cust_lists.update({cust_seg: (\n",
    "                self._tgt_group_info\n",
    "                .astype({\"資料開始週期\": str})\n",
    "                .set_index([\"客戶編碼\", \"資料開始週期\"])\n",
    "                .loc[(\n",
    "                    self._tgt_group_info\n",
    "                    .groupby(\"客戶編碼\")\n",
    "                    .agg({\"資料開始週期\": \"max\"})\n",
    "                    .astype({\"資料開始週期\": str})\n",
    "                    .to_records()\n",
    "                    .tolist()\n",
    "                )]\n",
    "                .reset_index()\n",
    "                .query(\"客戶分群 in @cust_seg_list\")\n",
    "                .客戶編碼\n",
    "                .values\n",
    "            )})\n",
    "\n",
    "        return self.cust_lists[cust_seg]\n",
    "\n",
    "    def _get_data(self, tgt: str):\n",
    "        logger.info(f'''days of t v t, {self.train_start_date}, {self.train_end_date}, \n",
    "        {self.tgt_date -timedelta(days=365)}, {self.tgt_date - timedelta(days=183)}, self.tgt_date''')\n",
    "        if tgt == \"train\":\n",
    "            df = self._create_cust_sunday_cartesian_df(self.vip_list,\n",
    "                                                       self.train_start_date,\n",
    "                                                       self.train_end_date)\n",
    "        elif tgt == \"val\":\n",
    "            df = self._create_cust_sunday_cartesian_df(self.vip_list,\n",
    "                                                       self.tgt_date -\n",
    "                                                       timedelta(days=365),\n",
    "                                                       self.tgt_date - timedelta(days=183))\n",
    "        elif tgt == \"test\":\n",
    "            df = self._create_cust_sunday_cartesian_df(self.vip_list,\n",
    "                                                       self.tgt_date,\n",
    "                                                       self.tgt_date)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown Target data\")\n",
    "        print('get data')\n",
    "        # embed()\n",
    "        return (\n",
    "            df\n",
    "            .assign(**self._create_txn_f(df, self.txn_c, \"C\"),  # 產生Credit相關特徵\n",
    "                    **self._create_txn_f(df, self.txn_d, \"D\"),  # 產生Debit相關特徵\n",
    "                    **self._create_txn_f(df, self.txn_t, \"T\"),  # 產生Txn相關特徵\n",
    "                    **self._add_latest_cust_info(df, self._risk_processed))  # 使用者最後狀態特徵\n",
    "            .pipe(lambda x: x.loc[:, ~x.columns.duplicated()])\n",
    "            .assign(**self._add_latest_cust_info(df, self._info_processed))\n",
    "            .pipe(lambda x: x.loc[:, ~x.columns.duplicated()])\n",
    "        )\n",
    "\n",
    "    def _create_cust_sunday_cartesian_df(self, cust, start_date, end_date):\n",
    "        sampled_dates = pd.date_range(\n",
    "            start_date, end_date)  # 產生一個包含從起始日到結束日的日期清單\n",
    "        # 從上面的清單中保留周日的部分\n",
    "        sampled_dates = sampled_dates[sampled_dates.isocalendar().day == 7]\n",
    "        sam_df = (\n",
    "            pd.DataFrame.from_records(\n",
    "                product(cust, sampled_dates),\n",
    "                columns=[self.entity_level, \"資料日期\"])\n",
    "        )\n",
    "        if self.level == \"ACC\":\n",
    "            sam_df = (\n",
    "                sam_df\n",
    "                .assign(**(\n",
    "                    sam_df\n",
    "                    .reset_index()\n",
    "                    .merge(self.ac, how=\"left\")\n",
    "                    .pipe(lambda x: (\n",
    "                        x.loc[\n",
    "                            x\n",
    "                            .query(\"資料日期 > 資料開始週期\")\n",
    "                            .groupby([\"帳戶編碼\", \"資料日期\"])\n",
    "                            [\"資料開始週期\"]\n",
    "                            .idxmax()\n",
    "                        ]\n",
    "                    ))\n",
    "                    .set_index('index')\n",
    "                    .sort_index()\n",
    "                )[[\"客戶編碼\"]])\n",
    "            )\n",
    "\n",
    "        return sam_df\n",
    "\n",
    "    def _create_txn_f(self, sam_df, txn_df, tran_type='C'):\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        logger.info(f\"tran_type {tran_type}\")\n",
    "        sam_grouped = (\n",
    "            sam_df\n",
    "            .assign(**{self.rd_ndays: (sam_df.資料日期 - timedelta(days=self.n_days))})\n",
    "            .groupby(self.entity_level, sort=False)\n",
    "        )  # entity_level 是acc或 pty\n",
    "        sam_df.to_csv(os.path.join(snapshot_folder, 'sam.csv'))\n",
    "        txn_df.to_csv(os.path.join(snapshot_folder, 'txn.csv'))\n",
    "        sam0 = sam_df.assign(**{'交易日期': (sam_df.資料日期 - timedelta(days=self.n_days-1)),\n",
    "                                '折台幣交易金額': 0})[[self.entity_level, '交易日期', '折台幣交易金額']]\n",
    "        sam1 = sam_df.assign(**{'交易日期': (sam_df.資料日期 - timedelta(days=self.n_days-2)),\n",
    "                                '折台幣交易金額': 1})[[self.entity_level, '交易日期', '折台幣交易金額']]\n",
    "        txn_df = pd.concat([txn_df, sam0, sam1]).reset_index(drop=True)\n",
    "        txn_df = txn_df.rename({\"折台幣交易金額\": f\"{tran_type}折台幣交易金額\"}, axis=1)\n",
    "        timeseries = (\n",
    "            txn_df\n",
    "            .sort_values(\"交易日期\")\n",
    "            .groupby(self.entity_level, sort=False)\n",
    "            # .pipe(lambda x: print(x))\n",
    "            .filter(lambda x: x.name in sam_grouped.groups.keys())\n",
    "            .groupby(self.entity_level, sort=False)\n",
    "            .apply(lambda x: self._find_past_txn(sam_grouped.get_group(x.name), x))\n",
    "        )\n",
    "        logger.info(f'timeseries.index.name  is {timeseries.index.name}')\n",
    "        timeseries.to_csv(os.path.join(snapshot_folder, 'timeseries.csv'))\n",
    "        timeseries = timeseries.reset_index(drop=True)\n",
    "        if len(timeseries):\n",
    "            extraction_settings = MinimalFCParameters()\n",
    "            timeseries_len_gt1 = (\n",
    "                timeseries[['id', 'time', f\"{tran_type}折台幣交易金額\"]]\n",
    "                .dropna()  # 有一些交易的交易金額為 NaN, 不使用 impute 填值\n",
    "                .groupby(\"id\")\n",
    "                .filter(lambda x: len(x) > 1)\n",
    "            )\n",
    "            #  (series, column_id='id', column_sort='date', column_value='participants')\n",
    "            X = extract_features(timeseries_len_gt1,\n",
    "                                 column_id='id',\n",
    "                                 column_sort='time',\n",
    "                                 #  kind_to_fc_parameters={\"temperature\": {\n",
    "                                 #      \"sum_values\": None, \"mean\": None}},\n",
    "                                 #  column_value='participants',\n",
    "                                 default_fc_parameters=extraction_settings,\n",
    "                                 n_jobs=2)\n",
    "            X.to_csv(os.path.join(snapshot_folder, 'x.csv'))\n",
    "        else:\n",
    "            X = pd.DataFrame(\n",
    "                columns=[\"{tran_type}折台幣交易金額__\" + f for f in MinimalFCParameters()])\n",
    "            # columns=[\"折台幣交易金額__\" + f for f in ['sum_values', \"mean\"]])\n",
    "        return X\n",
    "\n",
    "    def _add_latest_cust_info(self, sam_df, cust_info):\n",
    "        sam_grouped = (\n",
    "            sam_df\n",
    "            .groupby(\"客戶編碼\", sort=False)\n",
    "        )\n",
    "        tmp = (cust_info\n",
    "               .sort_values(\"資料開始週期\")\n",
    "               .groupby(\"客戶編碼\", sort=False)\n",
    "               .filter(lambda x: x.name in sam_grouped.groups.keys())\n",
    "               .groupby(\"客戶編碼\", sort=False)\n",
    "               .apply(lambda x: self._find_latest_cust_info(sam_grouped.get_group(x.name), x)))\n",
    "        if tmp.shape[0] == 0:\n",
    "            raise NonDataError('output of _add_latest_cust_info is empty')\n",
    "        return (\n",
    "            cust_info\n",
    "            .sort_values(\"資料開始週期\")\n",
    "            .groupby(\"客戶編碼\", sort=False)\n",
    "            .filter(lambda x: x.name in sam_grouped.groups.keys())\n",
    "            .groupby(\"客戶編碼\", sort=False)\n",
    "            .apply(lambda x: self._find_latest_cust_info(sam_grouped.get_group(x.name), x))\n",
    "            .droplevel(0)\n",
    "        )\n",
    "\n",
    "    def _find_past_txn(self, cust_sam_df, cust_txn_df):\n",
    "        \"\"\"\n",
    "        (Description)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cust_sam_df : pandas.DataFrame\n",
    "            (Description)\n",
    "        cust_txn_df : pandas.DataFrame\n",
    "            (Description)\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        index_array : pandas.DataFrame\n",
    "            (Description)\n",
    "\n",
    "        See Also\n",
    "        --------\n",
    "        (Description)\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        (Description)\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> \n",
    "        \"\"\"\n",
    "        ts = []\n",
    "        rds_n = np.searchsorted(\n",
    "            cust_txn_df[\"交易日期\"].values, cust_sam_df[self.rd_ndays].values, side='right')\n",
    "        rds = np.searchsorted(\n",
    "            cust_txn_df[\"交易日期\"].values, cust_sam_df[\"資料日期\"].values, side='right')\n",
    "        for i, r in enumerate(zip(rds_n, rds)):\n",
    "            if r[1] - r[0]:\n",
    "                t = cust_txn_df.iloc[slice(*r)].copy()\n",
    "                a = cust_sam_df.iloc[i]\n",
    "                t[\"id\"] = a.name\n",
    "                date_range = pd.date_range(\n",
    "                    a[self.rd_ndays], a[\"資料日期\"], freq='H').values\n",
    "                t[\"time\"] = np.searchsorted(date_range, t[\"交易日期\"].values)\n",
    "                ts.append(t)\n",
    "        if ts:\n",
    "            return pd.concat(ts, axis=0)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _find_latest_cust_info(self, cust_sam_df, cust_info):\n",
    "        idx = np.searchsorted(\n",
    "            cust_info[\"資料開始週期\"].values, cust_sam_df[\"資料日期\"].values, side='right') - 1\n",
    "\n",
    "        cust_info = (\n",
    "            cust_info\n",
    "            .iloc[idx]\n",
    "            .set_index(cust_sam_df.index)\n",
    "        )\n",
    "        cust_info.iloc[np.where(idx == -1)[0]] = np.nan\n",
    "\n",
    "        return cust_info\n",
    "\n",
    "\n",
    "# %%\n",
    "# % % time\n",
    "\n",
    "# dataset = Dataset(tgt_date=pd.to_datetime(\"2022-10-02\"),\n",
    "#                   scenario_name='TWNA1401',\n",
    "#                   group='Person',\n",
    "#                   level='ACC',\n",
    "#                   bankL=True)\n",
    "\n",
    "# %%\n",
    "# dataset.cust_seg_cust_list('CEH2')\n",
    "\n",
    "# %%\n",
    "# assert dataset.num_sam_dist([\"year\", \"week\"], on=\"all\").警示數量.sum() == sum(\n",
    "#     dataset.num_sam_dist([\"year\", \"week\"], on=\"all\", cust_seg=seg).警示數量.sum() for seg in dataset.seg_mapping[\"all\"])\n",
    "\n",
    "# %%\n",
    "# assert dataset.num_sam_dist([\"year\", \"week\"], on=\"vip\").警示數量.sum() == sum(\n",
    "#     dataset.num_sam_dist([\"year\", \"week\"], on=\"vip\", cust_seg=seg).警示數量.sum() for seg in dataset.seg_mapping[\"all\"])\n",
    "\n",
    "# %%\n",
    "# 'CEH1': ['CE1', 'CH1'],\n",
    "# 'CML1': ['CM1', 'CL1'],\n",
    "# 'CEH2': ['CE2', 'CH2'],\n",
    "# 'CML2': ['CM2', 'CL2'],\n",
    "# 'CEH3': ['CE3', 'CH3'],\n",
    "# 'CML3': ['CM3', 'CL3'],\n",
    "# 'C4': ['CE4', 'CH4', 'CM4', 'CL4']\n",
    "\n",
    "# display(\n",
    "#     dataset\n",
    "#     ._alert_processed\n",
    "#     .query(\"客戶編碼 == '.--.770:'\")\n",
    "#     .sort_values(\"資料日期\")\n",
    "# )\n",
    "\n",
    "# print(len(dataset.cust_seg_cust_list('CML1')))\n",
    "\n",
    "# display(\n",
    "#     dataset\n",
    "#     ._alert_processed\n",
    "#     .query(\"@dataset.train_start_date <= 資料日期 < @dataset.train_end_date\")\n",
    "#     .query(\"客戶分群 in ['CM1', 'CL1']\")\n",
    "#     .groupby([\"客戶編碼\"])\n",
    "#     .agg({\"資料日期\": \"count\"})\n",
    "#     .rename({\"資料日期\": \"警示數量\"}, axis=1)\n",
    "# )\n",
    "\n",
    "# display(\n",
    "#     dataset._tgt_group_info\n",
    "#     .groupby(\"客戶編碼\")\n",
    "#     .agg({\"資料開始週期\": \"max\"})\n",
    "#     .astype({\"資料開始週期\": str})\n",
    "#     .to_records()\n",
    "#     .tolist()\n",
    "# )\n",
    "\n",
    "# display(dataset._alert_processed.客戶編碼.map((\n",
    "#     dataset._tgt_group_info\n",
    "#     .astype({\"資料開始週期\": str})\n",
    "#     .set_index([\"客戶編碼\", \"資料開始週期\"])\n",
    "#     .loc[(\n",
    "#         dataset._tgt_group_info\n",
    "#         .groupby(\"客戶編碼\")\n",
    "#         .agg({\"資料開始週期\": \"max\"})\n",
    "#         .astype({\"資料開始週期\": str})\n",
    "#         .to_records()\n",
    "#         .tolist()\n",
    "#     )]\n",
    "#     .reset_index()\n",
    "# ).pipe(lambda x: dict(zip(x.客戶編碼, x.客戶分群)))))\n",
    "\n",
    "# 以現在 tgt_date 的角度有可能沒有客戶是這個 segmentation\n",
    "# print(dataset.cust_seg_cust_list('CEH2'))\n",
    "\n",
    "# for z in ['CEH1', 'CML1', 'CEH2', 'CML2', 'CEH3', 'CML3', 'C4']:\n",
    "#     print(z, np.intersect1d(dataset.vip_list, dataset.cust_seg_cust_list(z)), len(np.intersect1d(dataset.vip_list, dataset.cust_seg_cust_list(z))))\n",
    "\n",
    "# %% [markdown]\n",
    "# # training & inference\n",
    "\n",
    "# %%\n",
    "\n",
    "class EmptyTraining(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class NumSamV1:\n",
    "    def __init__(self, dataset: Dataset, load=False):\n",
    "        self.dataset = dataset\n",
    "        self.cols_dropped = [\"資料日期\", \"資料開始週期\", \"警示數量\", \"relation_duration\"]\n",
    "        package_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "        self.model_path = os.path.join(\n",
    "            package_dir, f'./models/{dataset.scenario_name}/{dataset.group}')\n",
    "        self.pics_path = os.path.join(\n",
    "            package_dir, f'./pics/{dataset.scenario_name}/{dataset.group}')\n",
    "        # self.model_path = f'./models/{dataset.scenario_name}/{dataset.group}'\n",
    "        # self.pics_path = f'./pics/{dataset.scenario_name}/{dataset.group}'\n",
    "        os.makedirs(self.model_path, exist_ok=True)\n",
    "        os.makedirs(self.pics_path, exist_ok=True)\n",
    "\n",
    "        if load:\n",
    "            self.models = [joblib.load(os.path.join(self.model_path, fn))\n",
    "                           for fn in sorted(os.listdir(self.model_path))]\n",
    "        else:\n",
    "            for fn in os.listdir(self.model_path):\n",
    "                os.remove(os.path.join(self.model_path, fn))\n",
    "            self.models = []\n",
    "            self._train_inference = None\n",
    "            self._train_27_models()\n",
    "        self.reg = LinearRegression()\n",
    "\n",
    "        self._train_inference = None\n",
    "        self._val_inference = None\n",
    "        self._test_inference = None\n",
    "        self.cust_seg = None\n",
    "\n",
    "    def _create_label(self, tgt: str, tgt_week: int):\n",
    "        tgt_dates = getattr(self.dataset, tgt).資料日期 + \\\n",
    "            timedelta(days=(tgt_week-1)*7+1)\n",
    "        return (\n",
    "            getattr(self.dataset, tgt)\n",
    "            .assign(**tgt_dates.dt.isocalendar(),\n",
    "                    month=tgt_dates.dt.month,\n",
    "                    quarter=tgt_dates.dt.quarter,\n",
    "                    hfy=tgt_dates.dt.quarter.isin([3, 4])+1,\n",
    "                    bigMon=tgt_dates.dt.month.isin([1, 3, 5, 7, 8, 10, 12]).astype(int))\n",
    "            .merge(self.dataset.num_sam_dist([\"客戶編碼\", \"year\", \"week\"], on=\"vip\"), how='left')\n",
    "            .fillna({\"警示數量\": 0})\n",
    "        )\n",
    "\n",
    "    def _create_pipe(self, X_train):\n",
    "        return make_pipeline(\n",
    "            TargetEncoder(cols=X_train.columns[X_train.dtypes == 'object'],\n",
    "                          handle_missing='return_nan'),\n",
    "            SimpleImputer(missing_values=np.nan, strategy='mean'),\n",
    "            XGBRegressor()\n",
    "        )\n",
    "\n",
    "    def _train_27_models(self):\n",
    "        for i in range(1, 28):\n",
    "            train = self._create_label(\"train\", i)\n",
    "            x_train, y_train = train.drop(\n",
    "                self.cols_dropped, axis=1), train.警示數量\n",
    "            # from IPython import embed\n",
    "            # embed()\n",
    "            model = self._create_pipe(x_train)\n",
    "            model.fit(x_train, y_train)\n",
    "            joblib.dump(model, os.path.join(\n",
    "                self.model_path, f\"{datetime.now().date()}_w{str(i).zfill(2)}.pkl\"), compress=3)\n",
    "            self.models.append(model)\n",
    "\n",
    "    def _fit_reg(self):\n",
    "        train_num_df = self.next_week(\"train\")\n",
    "        if train_num_df.shape[0] == 0:\n",
    "            raise EmptyTraining('empty train_num_df')\n",
    "\n",
    "        self.reg.fit(train_num_df[['week1_gt']].values,\n",
    "                     train_num_df.week1_total.values)\n",
    "\n",
    "    def inference(self, cust_seg: str):\n",
    "        logger.info('NumSamV1 inference')\n",
    "        self.cust_seg = cust_seg\n",
    "        for tgt in [\"train\", \"val\", \"test\"]:\n",
    "            y_trues = {}\n",
    "            y_preds = {}\n",
    "            for i, model in enumerate(self.models):\n",
    "                xy = self._create_label(tgt, i)\n",
    "                x, y = xy.drop(self.cols_dropped, axis=1), xy.警示數量\n",
    "                y_trues[f\"week{i+1}_gt\"] = y.values\n",
    "                y_preds[f\"week{i+1}_pred\"] = model.predict(x)\n",
    "\n",
    "            cust_list = self.dataset.cust_seg_cust_list(self.cust_seg)\n",
    "\n",
    "            setattr(self, f\"_{tgt}_inference\", (\n",
    "                getattr(self.dataset, tgt)\n",
    "                .assign(**y_trues,\n",
    "                        **y_preds)\n",
    "                .query(\"客戶編碼 in @cust_list\")\n",
    "            ))\n",
    "\n",
    "            if tgt == \"train\":\n",
    "                self._fit_reg()\n",
    "\n",
    "    def mse_for_27_models(self, tgt: str):\n",
    "        mse = []\n",
    "        for i in range(1, 28):\n",
    "            mse.append(mean_squared_error(getattr(self, f\"_{tgt}_inference\")[f\"week{i}_gt\"].values,\n",
    "                                          getattr(self, f\"_{tgt}_inference\")[f\"week{i}_pred\"].values))\n",
    "\n",
    "        return np.array(mse)\n",
    "\n",
    "    def mae_for_27_models(self, tgt: str):\n",
    "        mae = []\n",
    "        for i in range(1, 28):\n",
    "            mae.append(mean_absolute_error(getattr(self, f\"_{tgt}_inference\")[f\"week{i}_gt\"].values,\n",
    "                                           getattr(self, f\"_{tgt}_inference\")[f\"week{i}_pred\"].values))\n",
    "\n",
    "        return np.array(mae)\n",
    "\n",
    "    def next_week(self, tgt):\n",
    "        getattr(self, f\"_{tgt}_inference\").to_csv(\n",
    "            os.path.join(snapshot_folder, f\"_{tgt}_inference.csv\"))\n",
    "        return (\n",
    "            getattr(self, f\"_{tgt}_inference\")\n",
    "            .pipe(lambda x: x.assign(資料日期=x.資料日期 + timedelta(days=1)))\n",
    "            .pipe(lambda x: x.assign(**x.filter(regex=\"week.*_pred\").clip(0, 6)))\n",
    "            .groupby(\"資料日期\")\n",
    "            .agg({\"week1_pred\": \"sum\",\n",
    "                  \"week1_gt\": \"sum\"})\n",
    "            .pipe(lambda x: (x\n",
    "                             .assign(week1_total_pred=self._mult_num(x[[\"week1_pred\"]].values),\n",
    "                                     week1_total=(x\n",
    "                                                  .index.to_series()\n",
    "                                                  .dt.isocalendar()\n",
    "                                                  .merge(self.dataset.num_sam_dist([\"year\", \"week\"], on=\"all\", cust_seg=self.cust_seg), how=\"left\")\n",
    "                                                  .警示數量\n",
    "                                                  .fillna(0)\n",
    "                                                  .values))\n",
    "                             )\n",
    "                  )\n",
    "            [[\"week1_pred\", \"week1_total_pred\", \"week1_gt\", \"week1_total\"]]\n",
    "            .apply(lambda x: np.maximum(x, 0))\n",
    "            .round()\n",
    "            .astype(int)\n",
    "            .pipe(lambda x: x[[\"week1_pred\", \"week1_total_pred\"]] if tgt == \"test\" else x)\n",
    "        )\n",
    "\n",
    "    def this_period(self, tgt: str, period: str):\n",
    "        assert period in [\"month\", \"quarter\", \"hfy\", \"year\"]\n",
    "        # from IPython import embed\n",
    "        # embed()\n",
    "\n",
    "        def gen_key(x, period):\n",
    "            dic = {\"year\": x.index.to_series().dt.year}\n",
    "            if period == \"hfy\":\n",
    "                dic.update(\n",
    "                    {period: x.index.to_series().dt.quarter.isin([3, 4])+1})\n",
    "            elif period != \"year\":\n",
    "                dic.update({period: getattr(x.index.to_series().dt, period)})\n",
    "            return dic\n",
    "\n",
    "        if period == \"year\":\n",
    "            self.this_period(tgt, \"hfy\")\n",
    "\n",
    "        return (\n",
    "            getattr(self, f\"_{tgt}_inference\")\n",
    "            .groupby(\"資料日期\")\n",
    "            .apply(lambda x: pd.DataFrame.from_dict({f\"{period}_pred\": [self._this_period_agg(x, period=period, on=\"vip\")],\n",
    "                                                     f\"{period}_total_pred\": [self._this_period_agg(x, period=period, on=\"all\")]}))\n",
    "            .droplevel(1)\n",
    "            .pipe(lambda x: (x\n",
    "                             .assign(**{f\"{period}_gt\": (x\n",
    "                                                         .assign(**gen_key(x, period))\n",
    "                                                         .merge(self.dataset.num_sam_dist2(list(set([\"year\"]+[period])), on=\"vip\", cust_seg=self.cust_seg), how=\"left\")\n",
    "                                                         .警示數量\n",
    "                                                         .fillna(0)\n",
    "                                                         .values)},\n",
    "                                     **{f\"{period}_total\": (x\n",
    "                                                            .assign(**gen_key(x, period))\n",
    "                                                            .merge(self.dataset.num_sam_dist2(list(set([\"year\"]+[period])), on=\"all\", cust_seg=self.cust_seg), how=\"left\")\n",
    "                                                            .警示數量\n",
    "                                                            .fillna(0)\n",
    "                                                            .values)})\n",
    "                             )\n",
    "                  )\n",
    "            .round()\n",
    "            .astype(int)\n",
    "            .pipe(lambda x: x[[f\"{period}_pred\", f\"{period}_total_pred\"]] if tgt == \"test\" else x)\n",
    "        )\n",
    "\n",
    "    def _mult_num(self, x):\n",
    "        try:\n",
    "            return self.reg.predict(x)\n",
    "        except NotFittedError:\n",
    "            return -1\n",
    "\n",
    "    def _this_period_agg(self, tgt_date_pred_df, period, on: str, designated_date=\"\"):\n",
    "        tgt_date = tgt_date_pred_df.name\n",
    "        if designated_date:\n",
    "            tgt_date = pd.to_datetime(designated_date)\n",
    "\n",
    "        if period == \"hfy\":\n",
    "            if tgt_date.month >= 7:\n",
    "                start_period = pd.to_datetime(f\"{tgt_date.year}-07-01\")\n",
    "                end_period = pd.to_datetime(f\"{tgt_date.year}-12-31\")\n",
    "            else:\n",
    "                start_period = pd.to_datetime(f\"{tgt_date.year}-01-01\")\n",
    "                end_period = pd.to_datetime(f\"{tgt_date.year}-06-30\")\n",
    "        else:\n",
    "            start_period = tgt_date.to_period(\n",
    "                \"M\" if period == \"month\" else \"Y\" if period == \"year\" else \"Q\").start_time\n",
    "            end_period = tgt_date.to_period(\n",
    "                \"M\" if period == \"month\" else \"Y\" if period == \"year\" else \"Q\").end_time\n",
    "\n",
    "        sam_sum = 0\n",
    "\n",
    "        if period == \"year\":\n",
    "            sam_sum += self._this_period_agg(tgt_date_pred_df, \"hfy\", on)\n",
    "            if tgt_date.quarter in [1, 2]:\n",
    "                last_year_34 = self._this_period_agg(\n",
    "                    tgt_date_pred_df, \"hfy\", on, designated_date=f\"{tgt_date.year-1}-12-31\")\n",
    "                last_year_12 = self._this_period_agg(\n",
    "                    tgt_date_pred_df, \"hfy\", on, designated_date=f\"{tgt_date.year-1}-06-30\")\n",
    "                if last_year_12 > 0:\n",
    "                    ratio = last_year_34/last_year_12\n",
    "                    sam_sum *= (ratio+1)\n",
    "                else:\n",
    "                    sam_sum += last_year_34\n",
    "            else:\n",
    "                sam_sum += self._this_period_agg(\n",
    "                    tgt_date_pred_df, \"hfy\", on, designated_date=f\"{tgt_date.year}-06-30\")\n",
    "        else:\n",
    "            if tgt_date > start_period:\n",
    "                sam_sum += (\n",
    "                    pd.date_range(\n",
    "                        start_period, tgt_date - timedelta(days=0 if designated_date else 1)).isocalendar()\n",
    "                    .merge(self.dataset.num_sam_dist([\"year\", \"week\", \"day\"], on=on, cust_seg=self.cust_seg), how=\"left\")\n",
    "                    .警示數量\n",
    "                    .sum()\n",
    "                )\n",
    "                if designated_date:\n",
    "                    return sam_sum\n",
    "\n",
    "            days_till_end = pd.date_range(tgt_date, end_period).isocalendar()\n",
    "            num_week = len(days_till_end[[\"year\", \"week\"]].drop_duplicates())\n",
    "            if tgt_date <= end_period:\n",
    "                pred = (\n",
    "                    tgt_date_pred_df\n",
    "                    [[f\"week{i}_pred\" for i in range(1, num_week)]]\n",
    "                    .apply(lambda x: x.clip(0, 6))\n",
    "                    .sum().sum()\n",
    "                ) + (\n",
    "                    tgt_date_pred_df[f\"week{num_week}_pred\"]\n",
    "                    .clip(0, 6)\n",
    "                    .sum()\n",
    "                ) * (days_till_end.groupby([\"year\", \"week\"]).size().min() / 7.)\n",
    "\n",
    "                if on == \"all\":\n",
    "                    pred = np.maximum(self.reg.predict(\n",
    "                        np.array([[pred]]))[0], 0)\n",
    "\n",
    "                sam_sum += pred\n",
    "\n",
    "        return sam_sum\n",
    "\n",
    "    def save_png_ret_str(self):\n",
    "        (\n",
    "            pd.concat([self.next_week(\"train\"),\n",
    "                       self.next_week(\"val\")])\n",
    "            .rename_axis(\"run_date\", axis=0)\n",
    "        ).plot(marker=\".\")\n",
    "\n",
    "        plt.savefig(os.path.join(self.pics_path, \"pred_trend.png\"))\n",
    "\n",
    "        with open(os.path.join(self.pics_path, \"pred_trend.png\"), \"rb\") as f:\n",
    "            png_encoded = base64.b64encode(f.read())\n",
    "\n",
    "        return png_encoded.decode()\n",
    "\n",
    "\n",
    "# %%\n",
    "# model = NumSamV1(dataset, load=False)\n",
    "\n",
    "# %%\n",
    "# model.inference(cust_seg=\"all\")\n",
    "\n",
    "# %%\n",
    "# pd.concat([model.next_week(\"train\"), model.next_week(\"val\")]\n",
    "#           ).rename_axis(\"run_date\", axis=0).plot(marker=\".\")\n",
    "# plt.show()\n",
    "\n",
    "# %%\n",
    "# plt.plot(model.mse_for_27_models(\"train\"), label=\"train\")\n",
    "# plt.plot(model.mse_for_27_models(\"val\"), label=\"val\")\n",
    "# plt.xlabel(\"nth model\")\n",
    "# plt.ylabel(\"M S E\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# %%\n",
    "# plt.plot(model.mae_for_27_models(\"train\"), label=\"train\")\n",
    "# plt.plot(model.mae_for_27_models(\"val\"), label=\"val\")\n",
    "# plt.xlabel(\"nth model\")\n",
    "# plt.ylabel(\"M A E\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# %%\n",
    "# model.this_period(\"test\", \"hfy\")\n",
    "\n",
    "# %%\n",
    "\n",
    "group_dir = {'P': 'Person', 'C': 'Corp'}\n",
    "# %%\n",
    "# tgt date不能是周日以外的日期\n",
    "\n",
    "level_dir = {'TWNA1A01': \"PTY\", 'TWNAB101': \"PTY\", 'TWNA1401': \"ACC\"}\n",
    "\n",
    "\n",
    "def training(date=datetime.now(), seg_group=\"P\", snro_cd='TWNA1A01', bankL=True):\n",
    "    logger.info(f'-----------------start training------------------  \\\n",
    "                \\nscenario name: {snro_cd}   \\\n",
    "                \\ndate: {date}\\\n",
    "                \\nseg: {seg_group}\\\n",
    "                \\n-------------------------------------------------')\n",
    "    # try:\n",
    "    level = level_dir[snro_cd]\n",
    "    assert date.isocalendar()[2] == 7\n",
    "    group = group_dir[seg_group]\n",
    "    # if group == 'all':\n",
    "    #     group_list = ['Person', 'Corp']\n",
    "    # else:\n",
    "    #     group_list = [group]\n",
    "\n",
    "    result = {'tgt_date': date, 'seg_group': group}\n",
    "    # 對於需要 retrain 的模型只有兩個 自然人或法人\n",
    "    # 輸入起始日 tgt_date, 分群 group, 以及 snro_cd 回傳相應的資料\n",
    "    try:\n",
    "        dataset = Dataset(date, snro_cd, group, level,\n",
    "                          bankL=bankL, is_Train=True)\n",
    "    except NonDataError as err:\n",
    "        message = str(err) + '\\n' + traceback.format_exc()\n",
    "        result = {}\n",
    "        result['status_code'] = 0\n",
    "        result['message'] = message\n",
    "        return result\n",
    "\n",
    "    model = NumSamV1(dataset, load=False)\n",
    "    model.inference(cust_seg=\"all\")\n",
    "    result['score'] = {\n",
    "        'MSE': round(np.average(model.mse_for_27_models(\"val\")), 2),\n",
    "        'MAE': round(np.average(model.mae_for_27_models(\"val\")), 2)\n",
    "    }\n",
    "    result['pic'] = model.save_png_ret_str()\n",
    "    model_score = {}\n",
    "    for i in range(27):\n",
    "        tmp_dir = {'MAE': model.mae_for_27_models(\"val\")[i],\n",
    "                   'MSE': model.mse_for_27_models(\"val\")[i]}\n",
    "        model_score[i+1] = tmp_dir\n",
    "    result['individual_model_score'] = model_score\n",
    "    result['status_code'] = 1\n",
    "    result['message'] = 'success'\n",
    "    # except Exception as err:\n",
    "    #     message = str(err) + '\\n' + traceback.format_exc()\n",
    "    #     result = {'status_code': 0, 'message': message}\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def inference(date=datetime.now(), snro_cd='TWNA1A01', group=\"all\", bankL=True):\n",
    "    logger.info(f'----------------start inference------------------  \\\n",
    "                \\n scenario name: {snro_cd}   \\\n",
    "                \\n date: {date}\\\n",
    "                \\n seg: {group}\\\n",
    "                \\n-------------------------------------------------')\n",
    "    assert date.isocalendar()[2] == 7\n",
    "#     segs = {\n",
    "#         \"Person\": ['個人高風險','個人非高風險'],\n",
    "#         \"Corp\": ['個人高風險','個人非高風險']\n",
    "#     }\n",
    "    level = level_dir[snro_cd]\n",
    "    segs = {\n",
    "        \"Person\": ['PML', 'PEH'],\n",
    "        \"Corp\": ['CML1', 'CEH1', 'CML2', 'CEH2', 'CML3', 'CEH3', 'C4']\n",
    "    }\n",
    "\n",
    "    if group == 'all':\n",
    "        group_list = ['Person', 'Corp']\n",
    "    else:\n",
    "        group_list = [group]\n",
    "\n",
    "    result = {'tgt_date': date, 'seg_group': {}}\n",
    "\n",
    "    for grp in group_list:\n",
    "        try:\n",
    "            print(f'load {grp}')\n",
    "            dataset = Dataset(date, snro_cd, group=grp,\n",
    "                              level=level, bankL=bankL, is_Train=False)\n",
    "\n",
    "        except NonDataError as err:\n",
    "            print('NonDataError')\n",
    "            message = str(err) + '\\n' + traceback.format_exc()\n",
    "            for seg in segs[grp]:\n",
    "                print(seg, ' nondata')\n",
    "                result['seg_group'][seg] = {\n",
    "                    'next_week_predict': 0,\n",
    "                    'month_predict': 0,\n",
    "                    'season_predict': 0,\n",
    "                    'half_year_predict': 0,\n",
    "                    'year_predict': 0,\n",
    "                    'status_code': 0,\n",
    "                    'message': message}\n",
    "                print(seg, ' nondata done')\n",
    "            continue\n",
    "\n",
    "        for seg in segs[grp]:\n",
    "            logger.info(f' run {seg}')\n",
    "            if len(dataset.cust_seg_cust_list(seg)) != 0:\n",
    "\n",
    "                print(\n",
    "                    f'len(dataset.cust_seg_cust_list(seg)) == {len(dataset.cust_seg_cust_list(seg))}')\n",
    "                model = NumSamV1(dataset, load=True)\n",
    "                print(seg)\n",
    "                try:\n",
    "                    model.inference(seg)\n",
    "                    if model.next_week(\"test\").week1_total_pred.shape[0] == 0:\n",
    "\n",
    "                        raise Nonnextweek('week1_total_pred.shape is empty')\n",
    "                except EmptyTraining as err:\n",
    "                    logger.info(f'{err}')\n",
    "                    result['seg_group'][seg] = {\n",
    "                        'next_week_predict': 0,\n",
    "                        'month_predict': 0,\n",
    "                        'season_predict': 0,\n",
    "                        'half_year_predict': 0,\n",
    "                        'year_predict': 0,\n",
    "                        'status_code': 1,\n",
    "                        'message': f\"EmptyTraining data! {err}\"\n",
    "                    }\n",
    "                    logger.info(f\"no customer in {seg} at tgt_date!\")\n",
    "                    continue\n",
    "                except Nonnextweek as err:\n",
    "                    logger.info('len(dataset.cust_seg_cust_list(seg)) == 0')\n",
    "                    result['seg_group'][seg] = {\n",
    "                        'next_week_predict': 0,\n",
    "                        'month_predict': 0,\n",
    "                        'season_predict': 0,\n",
    "                        'half_year_predict': 0,\n",
    "                        'year_predict': 0,\n",
    "                        'status_code': 1,\n",
    "                        'message': f\"no customer in {seg} at tgt_date!{err}\"\n",
    "                    }\n",
    "                    logger.info(f\"no customer in {seg} at tgt_date!\")\n",
    "                    continue\n",
    "                except Exception as err:\n",
    "                    raise\n",
    "\n",
    "                result['seg_group'][seg] = {\n",
    "                    'next_week_predict': model.next_week(\"test\").week1_total_pred[0],\n",
    "                    'month_predict': model.this_period(\"test\", \"month\").month_total_pred[0],\n",
    "                    'season_predict': model.this_period(\"test\", \"quarter\").quarter_total_pred[0],\n",
    "                    'half_year_predict': model.this_period(\"test\", \"hfy\").hfy_total_pred[0],\n",
    "                    'year_predict': model.this_period(\"test\", \"year\").year_total_pred[0],\n",
    "                    'status_code': 1,\n",
    "                    'message': 'success'\n",
    "                }\n",
    "            else:\n",
    "                logger.info('len(dataset.cust_seg_cust_list(seg)) == 0')\n",
    "                result['seg_group'][seg] = {\n",
    "                    'next_week_predict': 0,\n",
    "                    'month_predict': 0,\n",
    "                    'season_predict': 0,\n",
    "                    'half_year_predict': 0,\n",
    "                    'year_predict': 0,\n",
    "                    'status_code': 1,\n",
    "                    'message': f\"len(dataset.cust_seg_cust_list(seg)) == 0! \"\n",
    "                }\n",
    "                logger.info(f\"no customer in {seg} at tgt_date!\")\n",
    "        # except Exception as err:\n",
    "        #     message = str(err) + '\\n' + traceback.format_exc()\n",
    "        #     result = {'status_code': 0, 'message': message}\n",
    "    return result\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "class P(pprint.PrettyPrinter):\n",
    "    def _format(self, object, *args, **kwargs):\n",
    "        if isinstance(object, str):\n",
    "            if len(object) > 20:\n",
    "                object = object[:20] + '...'\n",
    "        return pprint.PrettyPrinter._format(self, object, *args, **kwargs)\n",
    "\n",
    "\n",
    "# %%\n",
    "if __name__ == \"__main__\":\n",
    "    import json\n",
    "    with open(os.path.join(snapshot_folder, 'final_result.txt'), 'w', encoding='utf-8') as g:\n",
    "\n",
    "        the_date = \"2022-10-16\"\n",
    "        for i in range(1):\n",
    "            print(the_date)\n",
    "            for scenario in ['TWNA1A01', 'TWNAB101', 'TWNA1401']:\n",
    "\n",
    "                g.write(f'\\n{scenario} P start training  the_date: {the_date}')\n",
    "                a_time = datetime.now()\n",
    "                TWNA1A01_training_out = training(date=pd.to_datetime(the_date),\n",
    "                                                 snro_cd=scenario,\n",
    "                                                 seg_group='P',\n",
    "                                                 bankL=True)\n",
    "                TWNA1A01_training_out['tgt_date'] = str(\n",
    "                    TWNA1A01_training_out['tgt_date'])\n",
    "                with open(os.path.join(snapshot_folder, f'{the_date}_{scenario}_P_train_result.json'), 'w', encoding='utf-8') as h:\n",
    "                    json.dump(TWNA1A01_training_out, h)\n",
    "                g.write(\n",
    "                    f'\\n{scenario} P training finished  use time {(datetime.now()-a_time).total_seconds()}')\n",
    "                g.write(f\"\\n MSE: {TWNA1A01_training_out['score']['MSE']}\")\n",
    "                g.write(f\"\\n MSE: {TWNA1A01_training_out['score']['MSE']}\")\n",
    "\n",
    "                g.write(f'\\n{scenario} C start training  the_date: {the_date}')\n",
    "                a_time = datetime.now()\n",
    "                TWNA1A01_training_out = training(date=pd.to_datetime(the_date),\n",
    "                                                 snro_cd=scenario,\n",
    "                                                 seg_group='C',\n",
    "                                                 bankL=True)\n",
    "                TWNA1A01_training_out['tgt_date'] = str(\n",
    "                    TWNA1A01_training_out['tgt_date'])\n",
    "                with open(os.path.join(snapshot_folder, f'{the_date}_{scenario}_C_train_result.json'), 'w', encoding='utf-8') as h:\n",
    "                    json.dump(TWNA1A01_training_out, h)\n",
    "                g.write(\n",
    "                    f'\\n{scenario} P training finished. use time {(datetime.now()-a_time).total_seconds()}')\n",
    "                g.write(f\"\\n MSE: {TWNA1A01_training_out['score']['MSE']}\")\n",
    "                g.write(f\"\\n MSE: {TWNA1A01_training_out['score']['MSE']}\")\n",
    "\n",
    "                g.write(f'\\n{scenario} start inference  the_date: {the_date}')\n",
    "                a_time = datetime.now()\n",
    "                TWNA1A01_inference_out = inference(date=pd.to_datetime(the_date),\n",
    "                                                   snro_cd=scenario,\n",
    "                                                   group='all',\n",
    "                                                   bankL=True)\n",
    "                TWNA1A01_inference_out['tgt_date'] = str(\n",
    "                    TWNA1A01_inference_out['tgt_date'])\n",
    "                for seg in TWNA1A01_inference_out['seg_group'].keys():\n",
    "                    for x2 in TWNA1A01_inference_out['seg_group'][seg].keys():\n",
    "                        TWNA1A01_inference_out['seg_group'][seg][x2] = str(\n",
    "                            TWNA1A01_inference_out['seg_group'][seg][x2])\n",
    "                with open(os.path.join(snapshot_folder, f'{the_date}_{scenario}_inference_result.json'), 'w', encoding='utf-8') as h:\n",
    "                    json.dump(TWNA1A01_inference_out, h)\n",
    "\n",
    "                g.write(\n",
    "                    f'\\n{scenario} inference finished. use time {(datetime.now()-a_time).total_seconds()}.\\n the_date: {the_date}\\n {TWNA1A01_inference_out}')\n",
    "\n",
    "            the_date = datetime.strptime(\n",
    "                the_date, '%Y-%m-%d').date() + timedelta(days=7)\n",
    "            the_date = the_date.strftime('%Y-%m-%d')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
